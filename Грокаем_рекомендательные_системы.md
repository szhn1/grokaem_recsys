# Грокаем рекомендательные системы

## Содержание
1. [Введение в рекомендательные системы](#введение-в-рекомендательные-системы)
2. [Подготовка данных для рекомендательных систем](#подготовка-данных-для-рекомендательных-систем)
3. [Collaborative Filtering (Коллаборативная фильтрация)](#collaborative-filtering-коллаборативная-фильтрация)
4. [Content-Based Filtering (Контентная фильтрация)](#content-based-filtering-контентная-фильтрация)
5. [Гибридные подходы](#гибридные-подходы)
6. [Матричная факторизация](#матричная-факторизация)
7. [Метрики оценки рекомендательных систем](#метрики-оценки-рекомендательных-систем)
8. [Продвинутые подходы в рекомендательных системах](#продвинутые-подходы-в-рекомендательных-системах)
9. [Построение полноценной рекомендательной системы](#построение-полноценной-рекомендательной-системы)
10. [Специализированные библиотеки для рекомендательных систем](#специализированные-библиотеки-для-рекомендательных-систем)
    - [Библиотека Implicit](#библиотека-implicit)
    - [Библиотека Surprise](#библиотека-surprise)
    - [Библиотека LightFM](#библиотека-lightfm)

## Введение в рекомендательные системы

### Что такое рекомендательные системы?

Рекомендательные системы — это программы, которые предсказывают, какие предметы (фильмы, товары, песни и т.д.) могут понравиться пользователю на основе информации о пользователе и предметах.

Представь, что ты открываешь Netflix или YouTube. Как эти сервисы понимают, что тебе показать первым? Они используют рекомендательные системы, которые анализируют твои предыдущие просмотры, оценки и другие действия, чтобы предложить то, что тебе может понравиться.

### Где используются рекомендательные системы?

1. **Стриминговые сервисы**: Netflix, YouTube, Spotify
2. **Интернет-магазины**: Amazon, Ozon, Wildberries
3. **Социальные сети**: Instagram, Facebook, TikTok
4. **Новостные агрегаторы**: Google News, Яндекс.Новости
5. **Онлайн-реклама**: Google Ads, Facebook Ads

### Типы рекомендательных систем

Существует несколько основных типов рекомендательных систем:

1. **Collaborative Filtering (Коллаборативная фильтрация)** — рекомендации основаны на схожести пользователей или предметов. Например, "Пользователи, похожие на тебя, также смотрели..."

2. **Content-Based Filtering (Контентная фильтрация)** — рекомендации основаны на характеристиках предметов. Например, "Похожие фильмы того же жанра и режиссера..."

3. **Гибридные подходы** — комбинация разных методов для более точных рекомендаций.

4. **Контекстные рекомендации** — учитывают контекст (время, погоду, местоположение и т.д.).

### Задание №1: Знакомство с данными для рекомендательных систем

Давай посмотрим, как выглядят данные, используемые в рекомендательных системах. Создай простой Python-файл и выполни следующий код:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Создаем простую таблицу оценок пользователей фильмам
data = {
    'Пользователь': ['Алексей', 'Алексей', 'Алексей', 'Мария', 'Мария', 'Мария', 'Иван', 'Иван', 'Иван'],
    'Фильм': ['Титаник', 'Матрица', 'Властелин колец', 'Титаник', 'Матрица', 'Интерстеллар', 'Властелин колец', 'Интерстеллар', 'Матрица'],
    'Оценка': [5, 4, 5, 3, 5, 4, 5, 3, 2]
}

# Создаем DataFrame
ratings_df = pd.DataFrame(data)

# Выводим таблицу
print("Исходная таблица оценок:")
print(ratings_df)

# Преобразуем в матрицу пользователь-предмет
user_item_matrix = ratings_df.pivot_table(index='Пользователь', columns='Фильм', values='Оценка')
print("\nМатрица пользователь-предмет:")
print(user_item_matrix)

# Визуализируем матрицу
plt.figure(figsize=(10, 6))
sns.heatmap(user_item_matrix, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Оценка'})
plt.title('Матрица оценок пользователь-фильм')
plt.tight_layout()
plt.show()
```

**Что ты должен увидеть и понять:**
1. Таблица оценок представляет, какой пользователь какую оценку поставил какому фильму
2. Матрица пользователь-предмет — ключевая структура данных для рекомендательных систем
3. Пустые места (NaN) означают, что пользователь не оценивал этот фильм — наша задача будет заполнить эти пробелы предсказаниями!

## Подготовка данных для рекомендательных систем

### Источники данных

Для рекомендательных систем нам нужны различные типы данных:

1. **Явные обратные связи** (Explicit feedback):
   - Оценки пользователей (от 1 до 5 звезд)
   - Лайки/дизлайки
   - Отзывы и комментарии

2. **Неявные обратные связи** (Implicit feedback):
   - История просмотров
   - Клики
   - Время, проведенное на странице
   - Покупки

3. **Информация о пользователях**:
   - Демографические данные (возраст, пол)
   - Местоположение
   - Интересы

4. **Информация о предметах**:
   - Жанры фильмов
   - Характеристики товаров
   - Теги и категории

### Подготовка данных

Перед тем как строить рекомендательную систему, необходимо правильно подготовить данные:

1. **Обработка пропущенных значений**:
   - Матрицы пользователь-предмет часто разреженные (много пропусков)
   - Можно заполнять средними значениями или оставлять как есть для определенных алгоритмов

2. **Масштабирование**:
   - Нормализация оценок (например, от 0 до 1)
   - Z-масштабирование для учета разных "стилей" оценивания

3. **Разделение на обучающую и тестовую выборки**:
   - Важно проверять качество рекомендаций на новых данных

### Задание №2: Загрузка и подготовка реального набора данных MovieLens

Теперь перейдем к работе с реальным набором данных. MovieLens — один из самых популярных наборов данных для изучения рекомендательных систем.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Загрузим данные MovieLens (маленький датасет)
# Загрузить можно отсюда: https://grouplens.org/datasets/movielens/100k/
# или использовать команду:
# !wget https://files.grouplens.org/datasets/movielens/ml-100k.zip
# !unzip ml-100k.zip

# Путь к файлам (измени на свой, если нужно)
ratings_path = 'ml-100k/u.data'
movies_path = 'ml-100k/u.item'
users_path = 'ml-100k/u.user'

# Загрузка рейтингов
ratings_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv(ratings_path, sep='\t', names=ratings_cols)

# Загрузка информации о фильмах
movies_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 
               'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', 
               'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 
               'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 
               'Thriller', 'War', 'Western']
movies = pd.read_csv(movies_path, sep='|', names=movies_cols, encoding='latin-1')

# Загрузка информации о пользователях
users_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']
users = pd.read_csv(users_path, sep='|', names=users_cols)

# Базовый анализ данных
print(f"Всего оценок: {len(ratings)}")
print(f"Число пользователей: {ratings['user_id'].nunique()}")
print(f"Число фильмов: {ratings['movie_id'].nunique()}")
print(f"Средняя оценка: {ratings['rating'].mean():.2f}")

# Объединение данных
data = pd.merge(ratings, movies[['movie_id', 'title']], on='movie_id')
data = pd.merge(data, users[['user_id', 'gender', 'age']], on='user_id')

# Выведем первые 5 строк объединенных данных
print("\nПример объединенных данных:")
print(data.head())

# Посмотрим распределение оценок
plt.figure(figsize=(10, 6))
sns.histplot(data=data, x='rating', bins=5, kde=True)
plt.title('Распределение оценок')
plt.xlabel('Оценка')
plt.ylabel('Количество')
plt.show()

# Создадим матрицу пользователь-фильм (первые 100 пользователей и 100 фильмов для наглядности)
small_data = data[data['user_id'] <= 100]
small_data = small_data[small_data['movie_id'] <= 100]
user_movie_matrix = small_data.pivot_table(index='user_id', columns='title', values='rating')

# Построим тепловую карту
plt.figure(figsize=(20, 10))
sns.heatmap(user_movie_matrix.iloc[:20, :20], cmap='YlGnBu', annot=True, fmt=".1f")
plt.title('Матрица оценок пользователь-фильм (срез 20x20)')
plt.tight_layout()
plt.show()

# Анализ разреженности матрицы
sparsity = 100 * (1 - user_movie_matrix.count().sum() / (user_movie_matrix.shape[0] * user_movie_matrix.shape[1]))
print(f"\nРазреженность матрицы: {sparsity:.2f}%")
```

**Что нужно понять из этого задания:**
1. Реальные данные для рекомендательных систем часто очень разрежены (95-99% ячеек пусты)
2. Имеются различные типы данных: о пользователях, предметах и взаимодействиях
3. Важно объединять разные источники данных для создания полной картины

## Collaborative Filtering (Коллаборативная фильтрация)

Коллаборативная фильтрация — одна из самых популярных техник в рекомендательных системах. Она основана на идее, что пользователи, которые согласились в прошлом (оценили предметы схожим образом), скорее всего, согласятся и в будущем.

### Типы коллаборативной фильтрации

1. **User-Based Collaborative Filtering (основанная на пользователях)**:
   - Находим похожих пользователей
   - Рекомендуем предметы, которые понравились похожим пользователям

2. **Item-Based Collaborative Filtering (основанная на предметах)**:
   - Находим похожие предметы
   - Рекомендуем предметы, похожие на те, что понравились пользователю

### Как работает User-Based Collaborative Filtering

1. Вычисляем меру сходства между пользователями (например, корреляция Пирсона или косинусное сходство)
2. Для каждого пользователя находим N наиболее похожих пользователей
3. Предсказываем оценку для предмета, рассчитывая взвешенное среднее оценок похожих пользователей

### Задание №3: Реализация простой User-Based Collaborative Filtering

```python
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# Создадим простой пример данных
data = {
    'Пользователь': ['Алексей', 'Алексей', 'Алексей', 'Алексей', 
                     'Мария', 'Мария', 'Мария', 'Мария', 
                     'Иван', 'Иван', 'Иван', 'Иван', 
                     'Анна', 'Анна', 'Анна', 'Анна'],
    'Фильм': ['Титаник', 'Матрица', 'Властелин колец', 'Интерстеллар',
              'Титаник', 'Матрица', 'Властелин колец', 'Гарри Поттер',
              'Властелин колец', 'Интерстеллар', 'Матрица', 'Гарри Поттер',
              'Титаник', 'Матрица', 'Гарри Поттер', 'Интерстеллар'],
    'Оценка': [5, 4, 5, 3, 3, 5, 4, 5, 5, 3, 2, 4, 4, 2, 5, 4]
}

# Создаем DataFrame
ratings_df = pd.DataFrame(data)

# Преобразуем в матрицу пользователь-предмет
user_item_matrix = ratings_df.pivot_table(index='Пользователь', columns='Фильм', values='Оценка')
print("Матрица пользователь-предмет:")
print(user_item_matrix)

# Заполним пропущенные значения нулями для расчета сходства
user_item_matrix_filled = user_item_matrix.fillna(0)

# Вычислим матрицу сходства пользователей с помощью косинусного сходства
user_similarity = pd.DataFrame(
    cosine_similarity(user_item_matrix_filled),
    index=user_item_matrix.index,
    columns=user_item_matrix.index
)

print("\nМатрица сходства пользователей (косинусное сходство):")
print(user_similarity)

# Визуализируем матрицу сходства
plt.figure(figsize=(8, 6))
sns.heatmap(user_similarity, annot=True, cmap='YlGnBu')
plt.title('Косинусное сходство между пользователями')
plt.tight_layout()
plt.show()

# Функция для получения K самых похожих пользователей
def get_similar_users(user_id, user_similarity_df, k=2):
    """Находит k пользователей, наиболее похожих на указанного пользователя"""
    # Исключаем самого пользователя из результатов
    similar_users = user_similarity_df[user_id].drop(user_id)
    # Сортируем по убыванию сходства и берем top-k
    similar_users = similar_users.sort_values(ascending=False).head(k)
    return similar_users

# Функция для предсказания оценки
def predict_rating(user_id, item_id, user_similarity_df, user_item_matrix, k=2):
    """Предсказывает оценку пользователя user_id для предмета item_id"""
    # Если пользователь уже оценил предмет, вернем эту оценку
    if not pd.isna(user_item_matrix.loc[user_id, item_id]):
        return user_item_matrix.loc[user_id, item_id]
    
    # Получаем k наиболее похожих пользователей
    similar_users = get_similar_users(user_id, user_similarity_df, k)
    
    # Вычисляем предсказание как взвешенное среднее оценок похожих пользователей
    numerator = 0
    denominator = 0
    
    for sim_user, similarity in similar_users.items():
        # Проверяем, что похожий пользователь оценил этот предмет
        if not pd.isna(user_item_matrix.loc[sim_user, item_id]):
            # Взвешиваем оценку по сходству
            numerator += similarity * user_item_matrix.loc[sim_user, item_id]
            denominator += similarity
    
    # Если знаменатель равен нулю, вернем среднюю оценку по всему датасету
    if denominator == 0:
        return user_item_matrix.stack().mean()
    
    return numerator / denominator

# Предскажем несколько оценок
print("\nПримеры предсказаний:")
print(f"Предсказание для Алексея, фильм 'Гарри Поттер': {predict_rating('Алексей', 'Гарри Поттер', user_similarity, user_item_matrix):.2f}")
print(f"Предсказание для Марии, фильм 'Интерстеллар': {predict_rating('Мария', 'Интерстеллар', user_similarity, user_item_matrix):.2f}")

# Заполним все пропущенные оценки предсказаниями
predicted_matrix = user_item_matrix.copy()

for user in user_item_matrix.index:
    for item in user_item_matrix.columns:
        if pd.isna(user_item_matrix.loc[user, item]):
            predicted_matrix.loc[user, item] = predict_rating(user, item, user_similarity, user_item_matrix)

print("\nМатрица после предсказания всех оценок:")
print(predicted_matrix)

# Визуализируем матрицу предсказаний
plt.figure(figsize=(10, 6))
sns.heatmap(predicted_matrix, annot=True, fmt=".2f", cmap='YlGnBu')
plt.title('Матрица оценок с предсказаниями')
plt.tight_layout()
plt.show()

# Функция для получения топ-N рекомендаций для пользователя
def get_top_n_recommendations(user_id, predicted_matrix, user_item_matrix, n=3):
    """Возвращает n лучших рекомендаций для пользователя"""
    # Получаем предметы, которые пользователь еще не оценил
    user_ratings = user_item_matrix.loc[user_id]
    not_rated_items = user_ratings[user_ratings.isna()].index
    
    # Получаем предсказанные оценки для этих предметов
    predicted_ratings = predicted_matrix.loc[user_id, not_rated_items]
    
    # Сортируем по убыванию предсказанной оценки и берем top-n
    top_recommendations = predicted_ratings.sort_values(ascending=False).head(n)
    
    return top_recommendations

# Получим рекомендации для каждого пользователя
print("\nТоп-2 рекомендации для каждого пользователя:")
for user in user_item_matrix.index:
    recommendations = get_top_n_recommendations(user, predicted_matrix, user_item_matrix, 2)
    print(f"{user}: {', '.join([f'{item} ({score:.2f})' for item, score in recommendations.items()])}")
```

**Что нужно понять из этого задания:**
1. Как вычисляется сходство между пользователями
2. Как предсказываются оценки на основе оценок похожих пользователей
3. Как формируются рекомендации для конкретного пользователя

### Item-Based Collaborative Filtering

Item-Based Collaborative Filtering работает аналогично User-Based подходу, но вместо поиска похожих пользователей мы ищем похожие предметы.

Преимущества Item-Based подхода:
1. Лучше масштабируется (обычно предметов меньше, чем пользователей)
2. Более стабилен (предпочтения предметов меняются медленнее, чем предпочтения пользователей)

### Задание №4: Реализация простой Item-Based Collaborative Filtering

```python
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# Используем тот же пример данных из предыдущего задания
data = {
    'Пользователь': ['Алексей', 'Алексей', 'Алексей', 'Алексей', 
                     'Мария', 'Мария', 'Мария', 'Мария', 
                     'Иван', 'Иван', 'Иван', 'Иван', 
                     'Анна', 'Анна', 'Анна', 'Анна'],
    'Фильм': ['Титаник', 'Матрица', 'Властелин колец', 'Интерстеллар',
              'Титаник', 'Матрица', 'Властелин колец', 'Гарри Поттер',
              'Властелин колец', 'Интерстеллар', 'Матрица', 'Гарри Поттер',
              'Титаник', 'Матрица', 'Гарри Поттер', 'Интерстеллар'],
    'Оценка': [5, 4, 5, 3, 3, 5, 4, 5, 5, 3, 2, 4, 4, 2, 5, 4]
}

# Создаем DataFrame
ratings_df = pd.DataFrame(data)

# Преобразуем в матрицу пользователь-предмет
user_item_matrix = ratings_df.pivot_table(index='Пользователь', columns='Фильм', values='Оценка')
print("Матрица пользователь-предмет:")
print(user_item_matrix)

# Заполним пропущенные значения нулями для расчета сходства
user_item_matrix_filled = user_item_matrix.fillna(0)

# Вычислим матрицу сходства предметов с помощью косинусного сходства
# Обратите внимание, что мы транспонируем матрицу, чтобы вычислить сходство между предметами, а не пользователями
item_similarity = pd.DataFrame(
    cosine_similarity(user_item_matrix_filled.T),
    index=user_item_matrix.columns,
    columns=user_item_matrix.columns
)

print("\nМатрица сходства предметов (косинусное сходство):")
print(item_similarity)

# Визуализируем матрицу сходства предметов
plt.figure(figsize=(10, 8))
sns.heatmap(item_similarity, annot=True, cmap='YlGnBu')
plt.title('Косинусное сходство между предметами')
plt.tight_layout()
plt.show()

# Функция для получения K самых похожих предметов
def get_similar_items(item_id, item_similarity_df, k=2):
    """Находит k предметов, наиболее похожих на указанный предмет"""
    # Исключаем сам предмет из результатов
    similar_items = item_similarity_df[item_id].drop(item_id)
    # Сортируем по убыванию сходства и берем top-k
    similar_items = similar_items.sort_values(ascending=False).head(k)
    return similar_items

# Функция для предсказания оценки с помощью Item-Based Collaborative Filtering
def predict_rating_item_based(user_id, item_id, item_similarity_df, user_item_matrix, k=2):
    """Предсказывает оценку пользователя user_id для предмета item_id используя Item-Based CF"""
    # Если пользователь уже оценил предмет, вернем эту оценку
    if not pd.isna(user_item_matrix.loc[user_id, item_id]):
        return user_item_matrix.loc[user_id, item_id]
    
    # Получаем оценки пользователя для других предметов
    user_ratings = user_item_matrix.loc[user_id].dropna()
    
    # Если пользователь ничего не оценил, вернем среднюю оценку по всему датасету
    if len(user_ratings) == 0:
        return user_item_matrix.stack().mean()
    
    # Получаем похожие предметы для каждого оцененного предмета
    similar_items_scores = pd.Series(dtype='float64')
    
    for rated_item, rating in user_ratings.items():
        # Находим предметы, похожие на оцененный
        similar_items = get_similar_items(rated_item, item_similarity_df, k)
        
        # Если предмет, для которого делаем предсказание, среди похожих,
        # добавляем его сходство, взвешенное по оценке пользователя
        if item_id in similar_items.index:
            similarity = similar_items[item_id]
            similar_items_scores[rated_item] = similarity * rating
    
    # Если не нашли похожих предметов, вернем среднюю оценку пользователя
    if len(similar_items_scores) == 0 or similar_items_scores.sum() == 0:
        return user_ratings.mean()
    
    # Вычисляем взвешенное среднее сходства предметов и оценок пользователя
    return similar_items_scores.sum() / abs(similar_items_scores).sum()

# Предскажем несколько оценок
print("\nПримеры предсказаний (Item-Based):")
print(f"Предсказание для Алексея, фильм 'Гарри Поттер': {predict_rating_item_based('Алексей', 'Гарри Поттер', item_similarity, user_item_matrix):.2f}")
print(f"Предсказание для Марии, фильм 'Интерстеллар': {predict_rating_item_based('Мария', 'Интерстеллар', item_similarity, user_item_matrix):.2f}")

# Заполним все пропущенные оценки предсказаниями
predicted_matrix_item_based = user_item_matrix.copy()

for user in user_item_matrix.index:
    for item in user_item_matrix.columns:
        if pd.isna(user_item_matrix.loc[user, item]):
            predicted_matrix_item_based.loc[user, item] = predict_rating_item_based(user, item, item_similarity, user_item_matrix)

print("\nМатрица после предсказания всех оценок (Item-Based):")
print(predicted_matrix_item_based)

# Визуализируем матрицу предсказаний
plt.figure(figsize=(10, 6))
sns.heatmap(predicted_matrix_item_based, annot=True, fmt=".2f", cmap='YlGnBu')
plt.title('Матрица оценок с предсказаниями (Item-Based)')
plt.tight_layout()
plt.show()

# Получим рекомендации для каждого пользователя
print("\nТоп-2 рекомендации для каждого пользователя (Item-Based):")
for user in user_item_matrix.index:
    # Используем ту же функцию get_top_n_recommendations, что и раньше
    recommendations = get_top_n_recommendations(user, predicted_matrix_item_based, user_item_matrix, 2)
    print(f"{user}: {', '.join([f'{item} ({score:.2f})' for item, score in recommendations.items()])}")

# Сравним рекомендации User-Based и Item-Based
print("\nСравнение рекомендаций User-Based и Item-Based:")
for user in user_item_matrix.index:
    user_based_recs = get_top_n_recommendations(user, predicted_matrix, user_item_matrix, 2)
    item_based_recs = get_top_n_recommendations(user, predicted_matrix_item_based, user_item_matrix, 2)
    
    print(f"{user}:")
    print(f"  User-Based: {', '.join([f'{item} ({score:.2f})' for item, score in user_based_recs.items()])}")
    print(f"  Item-Based: {', '.join([f'{item} ({score:.2f})' for item, score in item_based_recs.items()])}")
```

**Что нужно понять из этого задания:**
1. Как вычисляется сходство между предметами
2. Как предсказываются оценки на основе оценок похожих предметов
3. Как различаются рекомендации, полученные методами User-Based и Item-Based CF

### Преимущества и недостатки коллаборативной фильтрации

**Преимущества:**
1. Не требует информации о содержании предметов
2. Может обнаруживать неочевидные связи между предметами
3. Качество рекомендаций улучшается с ростом числа пользователей и оценок

**Недостатки:**
1. **Проблема холодного старта**: трудно рекомендовать новым пользователям или новые предметы
2. **Разреженность**: большинство пользователей оценивают лишь небольшую часть предметов
3. **Масштабируемость**: сложно работать с большими объемами данных 

## Content-Based Filtering (Контентная фильтрация)

В отличие от коллаборативной фильтрации, которая опирается на схожести между пользователями или предметами, контентная фильтрация основывается на характеристиках самих предметов и профилях пользователей.

### Как работает контентная фильтрация

1. **Представление предметов**: Каждый предмет (фильм, книга, песня) представляется набором признаков.
   - Для фильмов это могут быть жанры, актеры, режиссеры
   - Для статей — ключевые слова, темы, авторы
   - Для товаров — категории, бренды, характеристики

2. **Профиль пользователя**: Создается профиль интересов пользователя на основе предметов, которые ему понравились.

3. **Сопоставление**: Новые предметы рекомендуются на основе сходства их признаков с профилем пользователя.

### Преимущества контентной фильтрации

1. **Нет проблемы холодного старта для предметов**: Новые предметы можно рекомендовать сразу, как только они появляются в системе.
2. **Прозрачность**: Пользователю можно объяснить, почему ему рекомендуется тот или иной предмет.
3. **Независимость от других пользователей**: Требуются только данные о самом пользователе.

### Задание №5: Реализация простой контентной фильтрации

```python
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# Создадим простой датасет с фильмами и их характеристиками
movies_data = {
    'Название': ['Титаник', 'Матрица', 'Властелин колец', 'Интерстеллар', 'Гарри Поттер', 'Аватар', 'Начало'],
    'Жанры': ['драма, романтика', 'фантастика, боевик', 'фэнтези, приключения', 'фантастика, драма', 
              'фэнтези, приключения', 'фантастика, приключения', 'фантастика, боевик'],
    'Режиссер': ['Джеймс Кэмерон', 'Вачовски', 'Питер Джексон', 'Кристофер Нолан', 
                 'Крис Коламбус', 'Джеймс Кэмерон', 'Кристофер Нолан'],
    'Год': [1997, 1999, 2001, 2014, 2001, 2009, 2010]
}

# Создаем DataFrame с фильмами
movies_df = pd.DataFrame(movies_data)
print("Датасет с фильмами:")
print(movies_df)

# Объединим всю текстовую информацию о фильме для создания "содержимого"
# Это упрощенный подход - в реальных системах можно использовать более сложные подходы
movies_df['Содержимое'] = movies_df['Жанры'] + ' ' + movies_df['Режиссер'] + ' ' + movies_df['Год'].astype(str)
print("\nСодержимое фильмов для анализа:")
print(movies_df[['Название', 'Содержимое']])

# Создаем TF-IDF векторизатор для преобразования текста в числовые векторы
tfidf = TfidfVectorizer(stop_words='english')

# Применяем векторизатор к содержимому фильмов
tfidf_matrix = tfidf.fit_transform(movies_df['Содержимое'])

# Выводим размеры матрицы TF-IDF
print(f"\nРазмеры TF-IDF матрицы: {tfidf_matrix.shape}")

# Посмотрим, какие признаки (слова) получились после векторизации
print("\nПрименяемые признаки (слова):")
feature_names = tfidf.get_feature_names_out()
print(feature_names)

# Преобразуем разреженную матрицу в DataFrame для наглядности
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=movies_df['Название'], columns=feature_names)
print("\nTF-IDF векторы фильмов (первые 5 признаков):")
print(tfidf_df.iloc[:, :5])

# Вычисляем косинусное сходство между фильмами
movie_similarity = pd.DataFrame(
    cosine_similarity(tfidf_matrix),
    index=movies_df['Название'],
    columns=movies_df['Название']
)

print("\nМатрица сходства фильмов (на основе содержимого):")
print(movie_similarity)

# Визуализируем матрицу сходства фильмов
plt.figure(figsize=(10, 8))
sns.heatmap(movie_similarity, annot=True, cmap='YlGnBu', fmt=".2f")
plt.title('Косинусное сходство между фильмами (контентная фильтрация)')
plt.tight_layout()
plt.show()

# Создадим профили пользователей на основе их оценок
# Для примера используем данные из предыдущего задания
user_ratings = {
    'Алексей': {'Титаник': 5, 'Матрица': 4, 'Властелин колец': 5, 'Интерстеллар': 3},
    'Мария': {'Титаник': 3, 'Матрица': 5, 'Властелин колец': 4, 'Гарри Поттер': 5},
    'Иван': {'Властелин колец': 5, 'Интерстеллар': 3, 'Матрица': 2, 'Гарри Поттер': 4},
    'Анна': {'Титаник': 4, 'Матрица': 2, 'Гарри Поттер': 5, 'Интерстеллар': 4}
}

# Функция для создания профиля пользователя на основе его оценок
def create_user_profile(user_ratings, movies_tfidf, normalize=True):
    """
    Создает профиль пользователя как взвешенную сумму TF-IDF векторов фильмов,
    которые пользователь оценил.
    
    Параметры:
    user_ratings (dict): Словарь оценок пользователя {название_фильма: оценка}
    movies_tfidf (DataFrame): TF-IDF векторы фильмов
    normalize (bool): Нормализовать ли профиль (по умолчанию True)
    
    Возвращает:
    profile (Series): Профиль пользователя как вектор весов признаков
    """
    profile = pd.Series(0, index=movies_tfidf.columns)
    
    # Рассчитываем профиль как взвешенную сумму TF-IDF векторов
    for movie, rating in user_ratings.items():
        if movie in movies_tfidf.index:
            # Используем оценку как вес для TF-IDF вектора фильма
            profile += rating * movies_tfidf.loc[movie]
    
    # Нормализуем профиль, если требуется
    if normalize and profile.sum() > 0:
        profile = profile / profile.sum()
    
    return profile

# Создаем профили для каждого пользователя
user_profiles = {}
for user, ratings in user_ratings.items():
    user_profiles[user] = create_user_profile(ratings, tfidf_df)

print("\nПримеры профилей пользователей (первые 5 признаков):")
for user, profile in user_profiles.items():
    print(f"{user}: {profile.iloc[:5].to_dict()}")

# Функция для рекомендации фильмов пользователю на основе его профиля
def recommend_movies_content_based(user_profile, movies_tfidf, n=3, exclude_rated=None):
    """
    Рекомендует фильмы на основе сходства между профилем пользователя и TF-IDF векторами фильмов.
    
    Параметры:
    user_profile (Series): Профиль пользователя
    movies_tfidf (DataFrame): TF-IDF векторы фильмов
    n (int): Количество рекомендаций
    exclude_rated (list): Список фильмов, которые следует исключить из рекомендаций (уже оцененные)
    
    Возвращает:
    recommendations (Series): Рекомендуемые фильмы с их оценками сходства
    """
    # Вычисляем сходство между профилем пользователя и каждым фильмом
    similarity = {}
    
    for movie in movies_tfidf.index:
        # Исключаем уже оцененные фильмы, если они указаны
        if exclude_rated and movie in exclude_rated:
            continue
        
        # Вычисляем косинусное сходство между профилем пользователя и TF-IDF вектором фильма
        similarity[movie] = cosine_similarity(
            [user_profile.values],
            [movies_tfidf.loc[movie].values]
        )[0][0]
    
    # Преобразуем в Series и сортируем по убыванию сходства
    similarity_series = pd.Series(similarity).sort_values(ascending=False)
    
    # Возвращаем top-n рекомендаций
    return similarity_series.head(n)

# Получаем рекомендации для каждого пользователя
print("\nРекомендации на основе контентной фильтрации:")
for user, profile in user_profiles.items():
    # Исключаем уже оцененные фильмы
    rated_movies = list(user_ratings[user].keys())
    
    # Получаем рекомендации
    recommendations = recommend_movies_content_based(
        profile, tfidf_df, n=2, exclude_rated=rated_movies
    )
    
    print(f"{user}: {', '.join([f'{movie} ({score:.2f})' for movie, score in recommendations.items()])}")
```

**Что нужно понять из этого задания:**
1. Как представлять предметы в виде числовых векторов признаков
2. Как создавать профили пользователей на основе их предпочтений
3. Как рекомендовать предметы, используя сходство между профилем пользователя и векторами предметов

### Проблемы и ограничения контентной фильтрации

1. **Ограниченность по разнообразию**: Система рекомендует предметы, похожие на те, что пользователь уже оценил, что может привести к "пузырю фильтров".

2. **Проблема холодного старта для пользователей**: Для новых пользователей сложно создать точный профиль без начальных оценок.

3. **Сложность извлечения признаков**: Для некоторых типов контента (например, изображений, видео) сложно автоматически извлечь значимые признаки.

4. **Субъективность оценки**: Некоторые аспекты предметов (например, качество, стиль) трудно представить формально.

### Улучшения для контентной фильтрации

1. **Использование более сложных методов извлечения признаков**:
   - Для текста: Word2Vec, BERT и другие модели на основе нейронных сетей
   - Для изображений: CNN (сверточные нейронные сети)
   - Для аудио: спектральные признаки, MFCCs

2. **Комбинирование с другими подходами**:
   - Дополнение контентной фильтрации коллаборативной для решения проблемы разнообразия
   - Использование контекстной информации (время, местоположение)

## Гибридные подходы

Гибридные рекомендательные системы объединяют разные подходы для получения более точных и разнообразных рекомендаций.

### Типы гибридных подходов

1. **Взвешенный подход**: Объединяет оценки из разных рекомендательных систем с использованием весов.

2. **Переключающийся подход**: Выбирает один из подходов в зависимости от ситуации (например, коллаборативная фильтрация для популярных предметов, контентная для нишевых).

3. **Каскадный подход**: Один алгоритм фильтрует предметы, а другой уточняет рейтинг оставшихся.

4. **Смешанный подход**: Показывает рекомендации из разных источников одновременно.

### Задание №6: Реализация простой гибридной рекомендательной системы

Давайте объединим коллаборативную и контентную фильтрацию:

```python
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# Используем объединенный датасет из предыдущих заданий
movies_data = {
    'Название': ['Титаник', 'Матрица', 'Властелин колец', 'Интерстеллар', 'Гарри Поттер', 'Аватар', 'Начало'],
    'Жанры': ['драма, романтика', 'фантастика, боевик', 'фэнтези, приключения', 'фантастика, драма', 
              'фэнтези, приключения', 'фантастика, приключения', 'фантастика, боевик'],
    'Режиссер': ['Джеймс Кэмерон', 'Вачовски', 'Питер Джексон', 'Кристофер Нолан', 
                 'Крис Коламбус', 'Джеймс Кэмерон', 'Кристофер Нолан'],
    'Год': [1997, 1999, 2001, 2014, 2001, 2009, 2010]
}

# Оценки пользователей
user_ratings_data = {
    'Пользователь': ['Алексей', 'Алексей', 'Алексей', 'Алексей', 
                     'Мария', 'Мария', 'Мария', 'Мария', 
                     'Иван', 'Иван', 'Иван', 'Иван', 
                     'Анна', 'Анна', 'Анна', 'Анна'],
    'Фильм': ['Титаник', 'Матрица', 'Властелин колец', 'Интерстеллар',
              'Титаник', 'Матрица', 'Властелин колец', 'Гарри Поттер',
              'Властелин колец', 'Интерстеллар', 'Матрица', 'Гарри Поттер',
              'Титаник', 'Матрица', 'Гарри Поттер', 'Интерстеллар'],
    'Оценка': [5, 4, 5, 3, 3, 5, 4, 5, 5, 3, 2, 4, 4, 2, 5, 4]
}

# Создаем DataFrame-ы
movies_df = pd.DataFrame(movies_data)
ratings_df = pd.DataFrame(user_ratings_data)

# 1. КОЛЛАБОРАТИВНАЯ ФИЛЬТРАЦИЯ (Item-Based)
# Создаем матрицу пользователь-предмет
user_item_matrix = ratings_df.pivot_table(index='Пользователь', columns='Фильм', values='Оценка')

# Заполняем пропущенные значения нулями для расчета сходства
user_item_matrix_filled = user_item_matrix.fillna(0)

# Вычисляем матрицу сходства предметов
item_similarity = pd.DataFrame(
    cosine_similarity(user_item_matrix_filled.T),
    index=user_item_matrix.columns,
    columns=user_item_matrix.columns
)

# Функция для получения рекомендаций на основе коллаборативной фильтрации
def get_cf_recommendations(user_id, user_item_matrix, item_similarity, n=3):
    """Возвращает рекомендации на основе Item-Based Collaborative Filtering"""
    # Получаем оценки пользователя
    user_ratings = user_item_matrix.loc[user_id].dropna()
    
    # Если пользователь ничего не оценил, возвращаем пустой словарь
    if len(user_ratings) == 0:
        return {}
    
    # Получаем все фильмы, которые пользователь еще не оценил
    not_rated = user_item_matrix.columns[~user_item_matrix.columns.isin(user_ratings.index)]
    
    # Для каждого фильма, который пользователь не оценил, считаем предсказание
    predictions = {}
    
    for item in not_rated:
        # Находим фильмы, похожие на текущий, среди оцененных пользователем
        similar_items = item_similarity[item][user_ratings.index]
        
        # Взвешиваем оценки пользователя по сходству фильмов
        weighted_ratings = similar_items * user_ratings
        
        # Если сумма весов ненулевая, считаем предсказание
        if np.sum(np.abs(similar_items)) > 0:
            predictions[item] = np.sum(weighted_ratings) / np.sum(np.abs(similar_items))
    
    # Сортируем предсказания и возвращаем топ-n
    return dict(sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:n])

# 2. КОНТЕНТНАЯ ФИЛЬТРАЦИЯ
# Объединяем всю текстовую информацию о фильме
movies_df['Содержимое'] = movies_df['Жанры'] + ' ' + movies_df['Режиссер'] + ' ' + movies_df['Год'].astype(str)

# Создаем TF-IDF векторы
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies_df['Содержимое'])
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=movies_df['Название'], columns=tfidf.get_feature_names_out())

# Функция для создания профиля пользователя
def create_user_profile(user_ratings, movies_tfidf):
    """Создает профиль пользователя на основе его оценок"""
    profile = pd.Series(0, index=movies_tfidf.columns)
    
    for movie, rating in user_ratings.items():
        if movie in movies_tfidf.index:
            profile += rating * movies_tfidf.loc[movie]
    
    if profile.sum() > 0:
        profile = profile / profile.sum()
    
    return profile

# Функция для получения рекомендаций на основе контентной фильтрации
def get_cb_recommendations(user_id, user_item_matrix, movies_tfidf, n=3):
    """Возвращает рекомендации на основе Content-Based Filtering"""
    # Получаем оценки пользователя
    user_ratings = user_item_matrix.loc[user_id].dropna().to_dict()
    
    # Создаем профиль пользователя
    user_profile = create_user_profile(user_ratings, movies_tfidf)
    
    # Вычисляем сходство между профилем пользователя и каждым фильмом
    similarity = {}
    
    for movie in movies_tfidf.index:
        # Исключаем уже оцененные фильмы
        if movie in user_ratings:
            continue
        
        # Вычисляем косинусное сходство
        similarity[movie] = cosine_similarity(
            [user_profile.values],
            [movies_tfidf.loc[movie].values]
        )[0][0]
    
    # Сортируем и возвращаем топ-n рекомендаций
    return dict(sorted(similarity.items(), key=lambda x: x[1], reverse=True)[:n])

# 3. ГИБРИДНАЯ СИСТЕМА
def get_hybrid_recommendations(user_id, alpha=0.5):
    """
    Возвращает гибридные рекомендации с весом alpha для коллаборативной фильтрации
    и весом (1-alpha) для контентной фильтрации
    """
    # Получаем рекомендации от обоих подходов
    cf_recs = get_cf_recommendations(user_id, user_item_matrix, item_similarity)
    cb_recs = get_cb_recommendations(user_id, user_item_matrix, tfidf_df)
    
    # Объединяем рекомендации
    hybrid_recs = {}
    
    # Все фильмы из обоих списков рекомендаций
    all_movies = set(list(cf_recs.keys()) + list(cb_recs.keys()))
    
    for movie in all_movies:
        # Если фильм рекомендован обоими подходами, взвешиваем оценки
        if movie in cf_recs and movie in cb_recs:
            hybrid_recs[movie] = alpha * cf_recs[movie] + (1 - alpha) * cb_recs[movie]
        # Если только коллаборативной фильтрацией
        elif movie in cf_recs:
            hybrid_recs[movie] = alpha * cf_recs[movie]
        # Если только контентной фильтрацией
        else:
            hybrid_recs[movie] = (1 - alpha) * cb_recs[movie]
    
    # Сортируем и возвращаем топ-3 рекомендации
    return dict(sorted(hybrid_recs.items(), key=lambda x: x[1], reverse=True)[:3])

# Получаем рекомендации для каждого пользователя с разными подходами
users = user_item_matrix.index.tolist()

print("Сравнение рекомендаций различных подходов:")
for user in users:
    print(f"\nПользователь: {user}")
    
    # Коллаборативная фильтрация
    cf_recs = get_cf_recommendations(user, user_item_matrix, item_similarity, 2)
    print(f"Collaborative Filtering: {', '.join([f'{movie} ({score:.2f})' for movie, score in cf_recs.items()])}")
    
    # Контентная фильтрация
    cb_recs = get_cb_recommendations(user, user_item_matrix, tfidf_df, 2)
    print(f"Content-Based Filtering: {', '.join([f'{movie} ({score:.2f})' for movie, score in cb_recs.items()])}")
    
    # Гибридный подход
    hybrid_recs = get_hybrid_recommendations(user)
    print(f"Hybrid (50/50): {', '.join([f'{movie} ({score:.2f})' for movie, score in hybrid_recs.items()])}")

# Визуализируем количество общих рекомендаций между разными подходами
def count_common_recommendations(users, alpha_values):
    """Подсчитывает, сколько общих рекомендаций между CF и CB при разных значениях alpha"""
    results = []
    
    for alpha in alpha_values:
        common_count = 0
        total_count = 0
        
        for user in users:
            cf_recs = set(get_cf_recommendations(user, user_item_matrix, item_similarity, 3).keys())
            cb_recs = set(get_cb_recommendations(user, user_item_matrix, tfidf_df, 3).keys())
            hybrid_recs = set(get_hybrid_recommendations(user, alpha).keys())
            
            # Сколько рекомендаций из гибридной системы есть в CF
            cf_common = len(hybrid_recs.intersection(cf_recs))
            # Сколько рекомендаций из гибридной системы есть в CB
            cb_common = len(hybrid_recs.intersection(cb_recs))
            
            common_count += cf_common + cb_common
            total_count += len(hybrid_recs) * 2  # Максимально возможное количество общих рекомендаций
        
        # Процент общих рекомендаций
        percentage = (common_count / total_count) * 100 if total_count > 0 else 0
        results.append((alpha, percentage))
    
    return results

# Тестируем гибридный подход с разными весами
alpha_values = [0.0, 0.25, 0.5, 0.75, 1.0]
common_recs = count_common_recommendations(users, alpha_values)

# Визуализируем результаты
alphas, percentages = zip(*common_recs)

plt.figure(figsize=(10, 6))
plt.plot(alphas, percentages, marker='o', linestyle='-', linewidth=2)
plt.xlabel('Вес коллаборативной фильтрации (alpha)')
plt.ylabel('% общих рекомендаций с CF и CB')
plt.title('Влияние веса (alpha) на гибридные рекомендации')
plt.grid(True)
plt.xticks(alphas)
plt.ylim(0, 100)
plt.show()
```

**Что нужно понять из этого задания:**
1. Как объединять разные подходы в рекомендательной системе
2. Влияние весов на гибридные рекомендации
3. Как каждый подход дополняет друг друга, устраняя недостатки

## Матричная факторизация

Матричная факторизация — это метод, который позволяет разложить матрицу оценок пользователей на два меньших матрицы: одна представляет пользователей, а другая — предметы.

### Задание №7: Реализация матричной факторизации

```python
import pandas as pd
import numpy as np
from sklearn.decomposition import NMF
import matplotlib.pyplot as plt
import seaborn as sns

# Создаем матрицу оценок пользователей
user_item_matrix = pd.DataFrame({
    'Пользователь': ['Алексей', 'Алексей', 'Алексей', 'Мария', 'Мария', 'Мария', 'Иван', 'Иван', 'Иван'],
    'Фильм': ['Титаник', 'Матрица', 'Властелин колец', 'Титаник', 'Матрица', 'Интерстеллар', 'Властелин колец', 'Интерстеллар', 'Матрица'],
    'Оценка': [5, 4, 5, 3, 5, 4, 5, 3, 2]
})

# Преобразуем данные в матрицу
user_item_matrix = user_item_matrix.pivot_table(index='Пользователь', columns='Фильм', values='Оценка')

# Применяем метод главных компонент (NMF) для факторизации матрицы
nmf = NMF(n_components=2, init='random')
W = nmf.fit_transform(user_item_matrix)
H = nmf.components_

# Преобразуем результаты обратно в матрицу оценок
reconstructed_matrix = W @ H

# Визуализируем исходную и восстановленную матрицы
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(user_item_matrix, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Оценка'})
plt.title('Исходная матрица оценок')

plt.subplot(1, 2, 2)
sns.heatmap(reconstructed_matrix, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Оценка'})
plt.title('Восстановленная матрица оценок')

plt.tight_layout()
plt.show()
```

**Что нужно понять из этого задания:**
1. Как использовать метод главных компонент для факторизации матрицы
2. Как восстанавливать исходную матрицу оценок на основе факторизованных матриц

## Метрики оценки рекомендательных систем

### Задание №8: Реализация метрик оценки рекомендательных систем

```python
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score

# Создаем матрицу оценок пользователей
user_item_matrix = pd.DataFrame({
    'Пользователь': ['Алексей', 'Алексей', 'Алексей', 'Мария', 'Мария', 'Мария', 'Иван', 'Иван', 'Иван'],
    'Фильм': ['Титаник', 'Матрица', 'Властелин колец', 'Титаник', 'Матрица', 'Интерстеллар', 'Властелин колец', 'Интерстеллар', 'Матрица'],
    'Оценка': [5, 4, 5, 3, 5, 4, 5, 3, 2]
})

# Преобразуем данные в матрицу
user_item_matrix = user_item_matrix.pivot_table(index='Пользователь', columns='Фильм', values='Оценка')

# Реализация метрики MSE
mse = mean_squared_error(user_item_matrix, reconstructed_matrix)
print(f"Mean Squared Error: {mse}")

# Реализация метрики MAE
mae = mean_absolute_error(user_item_matrix, reconstructed_matrix)
print(f"Mean Absolute Error: {mae}")

# Реализация метрики ROC AUC
roc_auc = roc_auc_score(user_item_matrix.values.flatten(), reconstructed_matrix.values.flatten())
print(f"ROC AUC Score: {roc_auc}")
```

**Что нужно понять из этого задания:**
1. Как использовать различные метрики для оценки качества рекомендательных систем
2. Как сравнивать результаты различных алгоритмов

## Продвинутые подходы в рекомендательных системах

### Задание №9: Реализация продвинутого подхода в рекомендательных системах

```python
import pandas as pd
import numpy as np
from sklearn.decomposition import NMF
import matplotlib.pyplot as plt
import seaborn as sns

# Создаем матрицу оценок пользователей
user_item_matrix = pd.DataFrame({
    'Пользователь': ['Алексей', 'Алексей', 'Алексей', 'Мария', 'Мария', 'Мария', 'Иван', 'Иван', 'Иван'],
    'Фильм': ['Титаник', 'Матрица', 'Властелин колец', 'Титаник', 'Матрица', 'Интерстеллар', 'Властелин колец', 'Интерстеллар', 'Матрица'],
    'Оценка': [5, 4, 5, 3, 5, 4, 5, 3, 2]
})

# Преобразуем данные в матрицу
user_item_matrix = user_item_matrix.pivot_table(index='Пользователь', columns='Фильм', values='Оценка')

# Применяем метод главных компонент (NMF) для факторизации матрицы
nmf = NMF(n_components=2, init='random')
W = nmf.fit_transform(user_item_matrix)
H = nmf.components_

# Преобразуем результаты обратно в матрицу оценок
reconstructed_matrix = W @ H

# Визуализируем исходную и восстановленную матрицы
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(user_item_matrix, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Оценка'})
plt.title('Исходная матрица оценок')

plt.subplot(1, 2, 2)
sns.heatmap(reconstructed_matrix, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Оценка'})
plt.title('Восстановленная матрица оценок')

plt.tight_layout()
plt.show()
```

**Что нужно понять из этого задания:**
1. Как использовать метод главных компонент для факторизации матрицы
2. Как восстанавливать исходную матрицу оценок на основе факторизованных матриц

## Построение полноценной рекомендательной системы

### Задание №10: Построение полноценной рекомендательной системы

```python
import pandas as pd
import numpy as np
from sklearn.decomposition import NMF
import matplotlib.pyplot as plt
import seaborn as sns

# Создаем матрицу оценок пользователей
user_item_matrix = pd.DataFrame({
    'Пользователь': ['Алексей', 'Алексей', 'Алексей', 'Мария', 'Мария', 'Мария', 'Иван', 'Иван', 'Иван'],
    'Фильм': ['Титаник', 'Матрица', 'Властелин колец', 'Титаник', 'Матрица', 'Интерстеллар', 'Властелин колец', 'Интерстеллар', 'Матрица'],
    'Оценка': [5, 4, 5, 3, 5, 4, 5, 3, 2]
})

# Преобразуем данные в матрицу
user_item_matrix = user_item_matrix.pivot_table(index='Пользователь', columns='Фильм', values='Оценка')

# Применяем метод главных компонент (NMF) для факторизации матрицы
nmf = NMF(n_components=2, init='random')
W = nmf.fit_transform(user_item_matrix)
H = nmf.components_

# Преобразуем результаты обратно в матрицу оценок
reconstructed_matrix = W @ H

# Визуализируем исходную и восстановленную матрицы
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(user_item_matrix, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Оценка'})
plt.title('Исходная матрица оценок')

plt.subplot(1, 2, 2)
sns.heatmap(reconstructed_matrix, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Оценка'})
plt.title('Восстановленная матрица оценок')

plt.tight_layout()
plt.show()
```

**Что нужно понять из этого задания:**
1. Как использовать метод главных компонент для факторизации матрицы
2. Как восстанавливать исходную матрицу оценок на основе факторизованных матриц

### Заключение

Рекомендательные системы — это мощный инструмент для улучшения пользовательского опыта и повышения удовлетворенности клиентов. В этой главе мы рассмотрели различные подходы к построению рекомендательных систем, от базовых до продвинутых.

Теперь, когда ты понимаешь основные принципы работы рекомендательных систем, ты можешь применять эти знания в реальных проектах и создавать эффективные и интересные рекомендации для пользователей.

Удачи в разработке рекомендательных систем!

## Специализированные библиотеки для рекомендательных систем

В этом разделе мы познакомимся с популярными библиотеками Python, которые специально созданы для разработки рекомендательных систем. Они значительно упрощают реализацию сложных алгоритмов и позволяют быстро создавать эффективные рекомендательные системы.

### Библиотека Implicit

Implicit — это библиотека Python, специально созданная для построения рекомендательных систем на основе неявных данных. А что такое "неявные данные"? Это информация о поведении пользователя: просмотры страниц, клики на товары, время прослушивания песен — то есть не прямые оценки, а косвенные признаки интереса.

#### Почему Implicit и когда её использовать?

В реальной жизни пользователи редко ставят оценки фильмам или товарам, но постоянно совершают действия: просматривают, покупают, кладут в корзину. Эти "неявные сигналы" и использует Implicit для построения рекомендаций.

Выбирайте Implicit в следующих случаях:

1. **У вас есть данные о поведении пользователей, но не об их оценках**. Например, история просмотров, покупок, кликов.

2. **Вам нужны быстрые и масштабируемые рекомендации**. Implicit отлично оптимизирована для больших данных и может обрабатывать миллионы пользователей и предметов.

3. **Вы хотите простое API без лишних сложностей**. Библиотека имеет интуитивно понятный интерфейс, похожий на scikit-learn.

4. **Вам важна производительность**. Библиотека использует оптимизированные алгоритмы на C++ и поддерживает многопоточность.

#### Установка и настройка Implicit

Установка Implicit проста, но может требовать компилятора C++ для сборки:

```bash
pip install implicit
```

Если возникают проблемы с компиляцией, можно использовать:

```bash
pip install implicit --no-cache-dir
```

Или для пользователей Windows:

```bash
pip install --only-binary :all: implicit
```

#### Как работает Implicit: простое объяснение

Главный алгоритм в Implicit — это **Alternating Least Squares (ALS)** или метод чередующихся наименьших квадратов. Звучит сложно, но его можно объяснить так:

1. **Представление в скрытом пространстве**: Каждый пользователь и каждый предмет представляются в виде вектора в скрытом пространстве факторов. Эти векторы можно представить как точки в многомерном пространстве.

2. **Скрытые характеристики**: Компоненты векторов отражают скрытые характеристики, которые не наблюдаются напрямую. Например, для фильмов это могут быть "степень драматичности", "количество экшена", "романтическая составляющая" и т.д.

3. **Чередующиеся шаги**:
   - Сначала фиксируем векторы предметов и оптимизируем векторы пользователей
   - Затем фиксируем векторы пользователей и оптимизируем векторы предметов
   - Повторяем этот процесс несколько раз

4. **Предсказание взаимодействий**: Чтобы предсказать, насколько пользователь заинтересован в предмете, вычисляем скалярное произведение их векторов. Чем больше значение, тем выше вероятность взаимодействия.

Математически это выглядит так:
- Пользователя представляет вектор `u`
- Предмет представляет вектор `i`
- Предсказание: `взаимодействие(u, i) = u · i` (скалярное произведение)

#### Подготовка данных для Implicit

Implicit ожидает данные в определённом формате. Давайте рассмотрим пример:

```python
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
import implicit

# Пример данных о прослушиваниях песен
plays_data = {
    'user_id': [1, 1, 2, 2, 2, 3, 3, 3, 4, 4],
    'song_id': [1, 2, 1, 3, 4, 1, 2, 5, 3, 4],
    'play_count': [10, 5, 15, 3, 8, 20, 7, 12, 9, 6]
}

# Создаем DataFrame
plays_df = pd.DataFrame(plays_data)

# Преобразуем данные в матрицу взаимодействий
# Строки - пользователи, столбцы - песни, значения - количество прослушиваний
user_song_matrix = csr_matrix((
    plays_df['play_count'],  # Значения
    (plays_df['user_id'],    # Строки
     plays_df['song_id'])    # Столбцы
))

print("Форма матрицы взаимодействий:", user_song_matrix.shape)
print("Количество ненулевых элементов:", user_song_matrix.nnz)
```

В этом примере:
1. Мы создаем матрицу "пользователь-песня", где значения — количество прослушиваний
2. Мы используем разреженную матрицу формата CSR (Compressed Sparse Row), которая эффективно хранит разреженные данные
3. Рассчитываем разреженность матрицы (процент нулевых элементов)

#### Преобразование данных: важные советы

1. **Обработка значений**: Часто имеет смысл преобразовать сырые значения (например, время просмотра) для улучшения результатов:

```python
# Логарифмическое преобразование для сглаживания выбросов
user_song_matrix.data = np.log1p(user_song_matrix.data)

# Альтернативно, можно использовать бинарное представление
# (есть взаимодействие или нет)
binary_matrix = (user_song_matrix > 0).astype(np.float32)
```

2. **Нормализация данных**: Для некоторых данных может потребоваться нормализация:

```python
# Нормализация по строкам (для каждого пользователя)
row_sums = user_song_matrix.sum(axis=1).A1
row_indices, col_indices = user_song_matrix.nonzero()
user_song_matrix.data /= row_sums[row_indices]
```

#### Шаг за шагом: создание рекомендательной системы с Implicit

##### Шаг 1: Обучение модели ALS

```python
# Создаем модель ALS
model = implicit.als.AlternatingLeastSquares(
    factors=50,           # Количество скрытых факторов
    regularization=0.1,   # Параметр регуляризации против переобучения
    iterations=20,        # Количество итераций обучения
    calculate_training_loss=True,  # Вычислять ли потери при обучении
    random_state=42,      # Для воспроизводимости
    num_threads=4         # Количество потоков для параллельных вычислений
)

# Обучаем модель
model.fit(user_song_matrix)

# Теперь модель готова для рекомендаций!
```

##### Шаг 2: Получение рекомендаций для пользователя

```python
# ID пользователя, для которого хотим получить рекомендации
user_id = 2

# Получаем N рекомендаций для пользователя
N = 5
recommended_ids, scores = model.recommend(
    userid=user_id,
    user_items=user_song_matrix[user_id],  # Что пользователь уже слушал
    N=N,
    filter_already_liked_items=True,      # Исключить уже прослушанные
    filter_items=None,                     # Дополнительная фильтрация предметов
    recalculate_user=True                  # Пересчитать вектор пользователя
)

print(f"Топ-{N} рекомендаций для пользователя {user_id}:")
for i, (song_id, score) in enumerate(zip(recommended_ids, scores), 1):
    print(f"{i}. Песня {song_id} (оценка: {score:.2f})")
```

##### Шаг 3: Поиск похожих предметов

Implicit также позволяет находить похожие предметы, что полезно для рекомендаций вида "Похожие товары":

```python
# ID песни, для которой ищем похожие
song_id = 1

# Получаем N наиболее похожих песен
N = 3
similar_ids, similarities = model.similar_items(
    itemid=song_id,
    N=N+1  # +1 потому что первым будет сам предмет
)

print(f"Песни, похожие на {song_id}:")
# Пропускаем первый результат, так как это сама песня
for i, (similar_id, similarity) in enumerate(zip(similar_ids[1:], similarities[1:]), 1):
    print(f"{i}. Песня {similar_id} (сходство: {similarity:.2f})")
```

#### Важные параметры модели и их настройка

При использовании Implicit есть несколько ключевых параметров, которые сильно влияют на качество рекомендаций:

1. **factors** (количество факторов):
   ```python
   model = implicit.als.AlternatingLeastSquares(factors=100)
   ```
   - **Что это**: Размерность скрытого пространства факторов
   - **Рекомендации**: Начните с 50-100 факторов
   - **Влияние**: Больше факторов = более сложная модель, но риск переобучения

2. **regularization** (регуляризация):
   ```python
   model = implicit.als.AlternatingLeastSquares(regularization=0.1)
   ```
   - **Что это**: Параметр для предотвращения переобучения
   - **Рекомендации**: Обычно 0.01-0.1
   - **Влияние**: Больше = меньше переобучение, но может не уловить сложные паттерны

3. **alpha** (вес уверенности):
   ```python
   model = implicit.als.AlternatingLeastSquares(alpha=40)
   ```
   - **Что это**: Коэффициент, определяющий уверенность в положительных примерах
   - **Рекомендации**: По умолчанию 40, но зависит от данных
   - **Влияние**: Выше = больше вес положительным (сильным) взаимодействиям

4. **iterations** (количество итераций):
   ```python
   model = implicit.als.AlternatingLeastSquares(iterations=20)
   ```
   - **Что это**: Сколько циклов обучения провести
   - **Рекомендации**: 15-30 итераций обычно достаточно
   - **Влияние**: Слишком мало = недообучение, слишком много = переобучение и лишнее время

5. **use_native** и **use_cg**:
   ```python
   model = implicit.als.AlternatingLeastSquares(use_native=True, use_cg=True)
   ```
   - **Что это**: Параметры для выбора алгоритма решения
   - **Рекомендации**: Обычно оставляйте `use_native=True` и `use_cg=True` для большинства задач

#### Практический пример: создание музыкального рекомендателя

Давайте создадим более реалистичный пример музыкального рекомендателя:

```python
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from implicit.als import AlternatingLeastSquares
import matplotlib.pyplot as plt
import seaborn as sns

# Загрузка данных (допустим, у нас есть CSV-файл с прослушиваниями)
# В реальности вы бы загрузили данные из файла примерно так:
# listens_df = pd.read_csv('user_song_listens.csv')

# Для примера создадим синтетические данные
np.random.seed(42)
n_users = 1000
n_songs = 2000
n_listens = 20000

users = np.random.randint(0, n_users, n_listens)
songs = np.random.randint(0, n_songs, n_listens)
# Создаем смещенное распределение для имитации реалистичных данных:
# - Некоторые песни гораздо популярнее других
# - Некоторые пользователи гораздо активнее других
play_counts = np.random.pareto(2, n_listens) + 1  # Парето распределение
play_counts = play_counts.astype(int)  # Преобразуем в целые числа

listens_df = pd.DataFrame({
    'user_id': users,
    'song_id': songs,
    'play_count': play_counts
})

# Удаляем дубликаты (если один пользователь слушал песню несколько раз)
listens_df = listens_df.groupby(['user_id', 'song_id']).sum().reset_index()

print(f"Данных о прослушиваниях: {len(listens_df)}")
print(f"Уникальных пользователей: {listens_df['user_id'].nunique()}")
print(f"Уникальных песен: {listens_df['song_id'].nunique()}")
print(f"Среднее количество прослушиваний: {listens_df['play_count'].mean():.2f}")

# Анализ распределения прослушиваний
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
listens_per_user = listens_df.groupby('user_id')['play_count'].sum()
sns.histplot(listens_per_user, kde=True, bins=30)
plt.title('Распределение прослушиваний по пользователям')
plt.xlabel('Количество прослушиваний')
plt.ylabel('Количество пользователей')
plt.xscale('log')

plt.subplot(1, 2, 2)
listens_per_song = listens_df.groupby('song_id')['play_count'].sum()
sns.histplot(listens_per_song, kde=True, bins=30)
plt.title('Распределение прослушиваний по песням')
plt.xlabel('Количество прослушиваний')
plt.ylabel('Количество песен')
plt.xscale('log')

plt.tight_layout()
plt.show()

# Создаем матрицу взаимодействий
# Можно также добавить информацию о песнях и пользователях через словари
songs_dict = {song_id: f"Song_{song_id}" for song_id in range(n_songs)}
users_dict = {user_id: f"User_{user_id}" for user_id in range(n_users)}

# Создаем разреженную матрицу "пользователь-песня"
user_song_matrix = csr_matrix((
    listens_df['play_count'].values,
    (listens_df['user_id'].values, listens_df['song_id'].values)
), shape=(n_users, n_songs))

print(f"Форма матрицы: {user_song_matrix.shape}")
print(f"Заполненность матрицы: {user_song_matrix.nnz / (n_users * n_songs) * 100:.4f}%")

# Для улучшения результатов применяем логарифмическое преобразование
# Это сглаживает различия между небольшими и большими значениями
user_song_matrix.data = np.log1p(user_song_matrix.data)

# Обучаем модель
model = AlternatingLeastSquares(
    factors=100,
    regularization=0.05,
    iterations=25,
    calculate_training_loss=True,
    num_threads=4,
    random_state=42
)

# Засекаем время обучения
import time
start_time = time.time()

model.fit(user_song_matrix)

print(f"Время обучения: {time.time() - start_time:.2f} секунд")

# Если model.calculate_training_loss=True, можно посмотреть историю потерь
if hasattr(model, 'item_factors') and hasattr(model, 'user_factors'):
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(model.iteration_loss) + 1), model.iteration_loss)
    plt.xlabel('Итерация')
    plt.ylabel('Потери')
    plt.title('Изменение функции потерь при обучении')
    plt.grid(True)
    plt.show()

# Функция для получения информативных рекомендаций
def get_recommendations_with_details(user_id, model, user_song_matrix, songs_dict, n=10):
    # Получаем рекомендации
    song_ids, scores = model.recommend(
        userid=user_id,
        user_items=user_song_matrix[user_id],
        N=n,
        filter_already_liked_items=True
    )
    
    # Добавляем информацию о песнях
    recommendations = []
    for song_id, score in zip(song_ids, scores):
        song_name = songs_dict.get(song_id, f"Song_{song_id}")
        recommendations.append({
            'song_id': song_id,
            'song_name': song_name,
            'score': score
        })
    
    return recommendations

# Получаем рекомендации для нескольких случайных пользователей
sample_users = np.random.choice(n_users, 3, replace=False)

for user_id in sample_users:
    print(f"\nРекомендации для пользователя {users_dict[user_id]}:")
    recommendations = get_recommendations_with_details(
        user_id, model, user_song_matrix, songs_dict, n=5
    )
    
    for i, rec in enumerate(recommendations, 1):
        print(f"{i}. {rec['song_name']} (оценка: {rec['score']:.2f})")
```

#### Оценка качества рекомендаций

Чтобы оценить качество рекомендаций, мы можем использовать разные метрики:

```python
# Разделяем данные на обучающую и тестовую выборки
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(listens_df, test_size=0.2, random_state=42)

# Создаем матрицы для обучения и тестирования
train_matrix = csr_matrix((
    np.log1p(train_df['play_count'].values),
    (train_df['user_id'].values, train_df['song_id'].values)
), shape=(n_users, n_songs))

test_matrix = csr_matrix((
    np.log1p(test_df['play_count'].values),
    (test_df['user_id'].values, test_df['song_id'].values)
), shape=(n_users, n_songs))

# Обучаем модель только на тренировочных данных
model = implicit.als.AlternatingLeastSquares(factors=100, regularization=0.05, iterations=25)
model.fit(train_matrix)

# Функция для вычисления метрики Precision@k
def precision_at_k(model, train_matrix, test_matrix, k=10):
    # Получаем всех пользователей из тестовой выборки
    test_users = np.unique(test_df['user_id'].values)
    
    precisions = []
    
    for user_id in test_users:
        # Получаем рекомендации для пользователя
        recommended_ids, _ = model.recommend(
            userid=user_id,
            user_items=train_matrix[user_id],
            N=k,
            filter_already_liked_items=True
        )
        
        # Получаем фактически прослушанные песни из тестовой выборки
        actual_ids = test_df[test_df['user_id'] == user_id]['song_id'].values
        
        # Вычисляем Precision@k
        if len(actual_ids) > 0:
            precision = len(set(recommended_ids) & set(actual_ids)) / k
            precisions.append(precision)
    
    # Возвращаем среднее значение Precision@k
    return np.mean(precisions) if precisions else 0

# Вычисляем Precision@5
precision_5 = precision_at_k(model, train_matrix, test_matrix, k=5)
print(f"Precision@5: {precision_5:.4f}")

# Функция для вычисления Mean Average Precision (MAP)
def mean_average_precision(model, train_matrix, test_matrix, k=10):
    test_users = np.unique(test_df['user_id'].values)
    average_precisions = []
    
    for user_id in test_users:
        # Получаем рекомендации
        recommended_ids, _ = model.recommend(
            userid=user_id,
            user_items=train_matrix[user_id],
            N=k,
            filter_already_liked_items=True
        )
        
        # Получаем фактические положительные примеры
        actual_ids = test_df[test_df['user_id'] == user_id]['song_id'].values
        
        if len(actual_ids) > 0:
            # Вычисляем Average Precision
            ap = 0
            hits = 0
            
            for i, rec_id in enumerate(recommended_ids):
                if rec_id in actual_ids:
                    hits += 1
                    ap += hits / (i + 1)
            
            if hits > 0:
                ap /= min(len(actual_ids), k)
                average_precisions.append(ap)
    
    # Возвращаем среднее значение
    return np.mean(average_precisions) if average_precisions else 0

# Вычисляем MAP@10
map_10 = mean_average_precision(model, train_matrix, test_matrix, k=10)
print(f"MAP@10: {map_10:.4f}")
```

#### Настройка параметров LightFM для лучших результатов

Ключевые параметры, которые влияют на качество рекомендаций:

```python
model = LightFM(
    # 1. Тип функции потерь
    loss='warp',  # Варианты: 'warp', 'bpr', 'warp-kos', 'logistic'
    
    # 2. Размерность латентного пространства
    no_components=30,  # Обычно от 20 до 100
    
    # 3. Скорость обучения
    learning_rate=0.05,  # Обычно 0.01-0.1
    
    # 4. Регуляризация для предотвращения переобучения
    user_alpha=0.0001,  # Регуляризация для пользователей
    item_alpha=0.0001,  # Регуляризация для предметов
    
    # 5. Максимальный ранг для WARP-функции потерь
    max_sampled=30,  # Для ускорения обучения (по умолчанию все предметы)
    
    # 6. Случайное инициализирование для воспроизводимости
    random_state=42
)
```

Рекомендации по выбору параметров:

1. **loss**:
   - 'warp': Лучший выбор для ранжирования и общих рекомендаций
   - 'bpr': Хорошо работает с неявными данными (просмотры, клики)
   - 'logistic': Для явных рейтингов (оценки 1-5)
   - 'warp-kos': Когда есть явные негативные примеры

2. **no_components**:
   - Начните с 30 и увеличивайте, если модель слишком простая
   - Уменьшайте, если модель переобучается

3. **Регуляризация**:
   - Увеличивайте, если модель переобучается
   - Уменьшайте, если модель слишком простая и недообучается

#### Примеры использования LightFM в реальных сценариях

##### Пример 1: Рекомендация книг с учётом жанров и демографии

```python
# Предположим, у нас есть данные о пользователях, книгах и их жанрах
# а также о демографических характеристиках пользователей

# 1. Создаем датасет и подготавливаем данные
# ... (код из примера выше) ...

# 2. Обучаем модель
model = LightFM(loss='warp', no_components=40, learning_rate=0.05)
model.fit(
    interactions=interactions_matrix,
    user_features=user_features_matrix,
    item_features=item_features_matrix,
    epochs=50
)

# 3. Находим похожие книги (item-to-item рекомендации)
def get_similar_items(model, item_id, item_features, N=5):
    # Получаем индекс предмета
    item_mapping = dataset.mapping()[2]
    item_idx = item_mapping[item_id]
    
    # Получаем количество предметов
    n_items = len(item_mapping)
    
    # Получаем представление предметов из модели
    # (bias и representation)
    _, item_representations = model.get_item_representations(features=item_features)
    
    # Вычисляем косинусное сходство между текущим предметом и всеми остальными
    item_vector = item_representations[item_idx]
    similarities = item_representations.dot(item_vector) / (
        np.linalg.norm(item_representations, axis=1) * np.linalg.norm(item_vector)
    )
    
    # Исключаем сам предмет из результатов
    similarities[item_idx] = -np.inf
    
    # Берем топ-N похожих предметов
    top_similar = np.argsort(-similarities)[:N]
    
    # Преобразуем индексы обратно в ID
    reverse_mapping = {idx: item for item, idx in item_mapping.items()}
    
    return [(reverse_mapping[idx], similarities[idx]) for idx in top_similar]

# Находим книги, похожие на "Книга2"
similar_books = get_similar_items(model, 'Книга2', item_features_matrix)
print("Книги, похожие на 'Книга2':")
for book, similarity in similar_books:
    print(f"- {book} (сходство: {similarity:.2f})")
```

##### Пример 2: Рекомендации новым пользователям (холодный старт)

```python
# Предположим, у нас появился новый пользователь, о котором мы знаем только демографические данные
new_user = 'Елена'
new_user_features = ['женщина', 'молодая', 'технологии']

# 1. Добавляем пользователя в датасет
# В реальном сценарии нам нужно пересоздать датасет или использовать более сложную логику
# Для простоты примера, просто добавим признаки нового пользователя

# Создаем матрицу признаков для нового пользователя
new_user_features_dict = {feature: 1.0 for feature in new_user_features}
new_user_features_matrix = dataset.build_user_features(
    [(new_user, new_user_features)]
)

# 2. Получаем рекомендации для нового пользователя
# Для этого нам нужно получить индекс пользователя
user_mapping = dataset.mapping()[0]
if new_user in user_mapping:
    new_user_idx = user_mapping[new_user]
    
    # Получаем количество предметов
    n_items = len(dataset.mapping()[2])
    
    # Получаем предсказания для всех предметов
    scores = model.predict(
        user_ids=[new_user_idx] * n_items,
        item_ids=np.arange(n_items),
        user_features=new_user_features_matrix,
        item_features=item_features_matrix
    )
    
    # Сортируем предметы по предсказанным оценкам
    top_items = np.argsort(-scores)[:5]
    
    # Преобразуем индексы обратно в ID предметов
    item_mapping = {idx: item for item, idx in dataset.mapping()[2].items()}
    
    print(f"Рекомендации для нового пользователя {new_user}:")
    for item_idx in top_items:
        item_id = item_mapping[item_idx]
        print(f"- {item_id} (оценка: {scores[item_idx]:.2f})")
```

#### Решение типичных проблем при работе с LightFM

##### Проблема 1: Низкое качество рекомендаций

**Причины и решения:**

1. **Недостаточное количество данных для обучения**:
   - Добавьте больше взаимодействий, если это возможно
   - Используйте аугментацию данных или синтетические данные
   - Уменьшите размерность модели (`no_components`)

2. **Неподходящие признаки**:
   - Пересмотрите выбор признаков
   - Добавьте более информативные признаки
   - Попробуйте инженерию признаков (объединение, преобразование)

3. **Неправильно выбранные гиперпараметры**:
   - Проведите подбор гиперпараметров (grid search или random search)
   - Попробуйте разные функции потерь
   - Настройте регуляризацию и скорость обучения

##### Проблема 2: Переобучение модели

**Симптомы:** Модель отлично работает на тренировочных данных, но плохо на тестовых.

**Решения:**

1. Увеличьте параметры регуляризации (`user_alpha` и `item_alpha`)
2. Уменьшите количество эпох обучения
3. Уменьшите размерность модели (`no_components`)
4. Используйте ранний останов (early stopping)

##### Проблема 3: Медленное обучение

**Решения:**

1. Увеличьте число потоков (`num_threads`)
2. Уменьшите размер данных (выборка)
3. Уменьшите `max_sampled` для методов WARP и BPR
4. Используйте более быструю функцию потерь (например, logistic вместо warp)
5. Уменьшите количество признаков

#### Сравнение с другими библиотеками

**LightFM vs Implicit:**

- **LightFM:** Позволяет использовать признаки пользователей и предметов, решает проблему холодного старта
- **Implicit:** Быстрее работает на больших данных, но требует перестроения модели для новых пользователей

**LightFM vs Surprise:**

- **LightFM:** Лучше работает с неявными данными, поддерживает признаки
- **Surprise:** Предлагает больше классических алгоритмов, лучше работает с явными рейтингами

#### Заключение

LightFM — мощный инструмент для создания гибридных рекомендательных систем, который особенно полезен, когда:

1. У вас есть богатая информация о пользователях и предметах
2. Вам нужно решить проблему холодного старта
3. Вы хотите объединить преимущества коллаборативной и контентной фильтрации в одной модели

Начните с простой модели, постепенно добавляйте признаки и настраивайте параметры, и вы сможете создать эффективную рекомендательную систему, адаптированную под ваши конкретные задачи.

### Сравнение библиотек и выбор инструмента

| Библиотека | Сильные стороны | Слабые стороны | Лучший сценарий использования |
|------------|-----------------|----------------|------------------------------|
| **Implicit** | Высокая скорость, оптимизирована для неявных обратных связей, хорошая масштабируемость | Ограниченный набор алгоритмов, только для неявных обратных связей | Большие коммерческие системы с данными просмотров/покупок |
| **Surprise** | Разнообразие алгоритмов, простота использования, встроенная оценка моделей | Менее оптимизирована для больших данных, фокус на явных обратных связях | Исследования и эксперименты с разными алгоритмами, небольшие/средние наборы данных |
| **LightFM** | Гибридный подход, поддержка признаков, хорошая работа при холодном старте | Сложнее в настройке, требует больше подготовки данных | Системы с дополнительной информацией о пользователях/предметах, когда важно решать проблему холодного старта |

#### Рекомендации по выбору библиотеки

1. **Выбирайте Implicit, если**:
   - У вас данные неявных обратных связей (просмотры, клики, покупки)
   - Вам нужна высокая производительность на больших данных
   - Вы готовы ограничиться несколькими алгоритмами, но хотите быструю работу

2. **Выбирайте Surprise, если**:
   - У вас данные явных обратных связей (оценки, рейтинги)
   - Вам нужно экспериментировать с разными алгоритмами и проводить кросс-валидацию
   - Вам важен простой и понятный API, похожий на scikit-learn

3. **Выбирайте LightFM, если**:
   - У вас есть дополнительная информация о пользователях и предметах
   - Вам нужно решать проблему холодного старта
   - Вы хотите объединить преимущества коллаборативной и контентной фильтрации

### Задание №11: Сравнение библиотек на одном наборе данных

Попробуйте реализовать рекомендательную систему для одного и того же набора данных с использованием всех трех библиотек и сравните результаты. Это поможет вам лучше понять сильные и слабые стороны каждой библиотеки.

```python
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix

# Загрузим небольшой набор данных MovieLens для тестирования
# !wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip
# !unzip ml-latest-small.zip

# Загрузка данных
ratings = pd.read_csv('ml-latest-small/ratings.csv')
movies = pd.read_csv('ml-latest-small/movies.csv')

# Объединим данные о рейтингах и фильмах
data = pd.merge(ratings, movies, on='movieId')

# Создадим словари для маппинга пользователей и фильмов
user_ids = {user: i for i, user in enumerate(data['userId'].unique())}
movie_ids = {movie: i for i, movie in enumerate(data['movieId'].unique())}

# Обратные словари для интерпретации результатов
user_map_reverse = {v: k for k, v in user_ids.items()}
movie_map_reverse = {v: k for k, v in movie_ids.items()}

# Преобразуем данные для использования в разных библиотеках
rows = data['userId'].map(lambda x: user_ids[x])
cols = data['movieId'].map(lambda x: movie_ids[x])
ratings_values = data['rating'].values

# Создадим разреженную матрицу взаимодействий
sparse_matrix = csr_matrix((ratings_values, (rows, cols)), shape=(len(user_ids), len(movie_ids)))

# Разделим данные на обучающую и тестовую выборки
np.random.seed(42)
mask = np.random.rand(len(data)) < 0.8
train_data = data[mask]
test_data = data[~mask]

print(f"Размер тренировочного набора: {len(train_data)}")
print(f"Размер тестового набора: {len(test_data)}")

# 1. Implicit --------------------------------------------------------------
from implicit.als import AlternatingLeastSquares
import time

print("\n--- Тестирование Implicit ---")
# Преобразуем данные для Implicit
train_rows = train_data['userId'].map(lambda x: user_ids[x])
train_cols = train_data['movieId'].map(lambda x: movie_ids[x])
train_ratings = train_data['rating'].values

train_sparse = csr_matrix((train_ratings, (train_rows, train_cols)), shape=(len(user_ids), len(movie_ids)))

# Замеряем время обучения
start_time = time.time()

# Создаем и обучаем модель ALS
implicit_model = AlternatingLeastSquares(factors=50, regularization=0.1, iterations=30)
implicit_model.fit(train_sparse)

implicit_time = time.time() - start_time
print(f"Время обучения Implicit: {implicit_time:.2f} секунд")

# Оценим качество модели на тестовых данных
def implicit_rmse(model, test_data, user_map, movie_map):
    errors = []
    for _, row in test_data.iterrows():
        user_idx = user_map[row['userId']]
        movie_idx = movie_map[row['movieId']]
        
        # Предсказываем оценку
        pred = model.predict(user_idx, train_sparse[user_idx], [movie_idx])[0][1]
        errors.append((pred - row['rating']) ** 2)
    
    return np.sqrt(np.mean(errors))

implicit_rmse_score = implicit_rmse(implicit_model, test_data, user_ids, movie_ids)
print(f"RMSE для Implicit: {implicit_rmse_score:.4f}")

# 2. Surprise --------------------------------------------------------------
from surprise import Dataset, Reader
from surprise import SVD, KNNBasic, NMF
from surprise.model_selection import cross_validate, train_test_split as surprise_train_test_split
from surprise.accuracy import rmse, mae

print("\n--- Тестирование Surprise ---")
# Преобразуем данные для Surprise
reader = Reader(rating_scale=(0.5, 5.0))
surprise_data = Dataset.load_from_df(data[['userId', 'movieId', 'rating']], reader)

# Разделим данные на обучающую и тестовую выборки
trainset, testset = surprise_train_test_split(surprise_data, test_size=0.2, random_state=42)

# Замеряем время обучения
start_time = time.time()

# Создаем и обучаем модели
svd = SVD()
knn = KNNBasic()
nmf = NMF()

# Обучаем SVD модель
svd.fit(trainset)

# Оцениваем модель на тестовых данных
predictions = svd.test(testset)
print(f"RMSE для SVD: {rmse(predictions):.4f}")
print(f"MAE для SVD: {mae(predictions):.4f}")

# Кросс-валидация для сравнения моделей
print("\nКросс-валидация для разных алгоритмов:")

algos = [SVD(), KNNBasic(), NMF()]
names = ['SVD', 'KNN', 'NMF']

for algo, name in zip(algos, names):
    results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3, verbose=False)
    print(f"{name}: RMSE = {results['test_rmse'].mean():.4f}, MAE = {results['test_mae'].mean():.4f}")

# Предсказания для конкретного пользователя и фильма
user = 'Алексей'
item = 'Интерстеллар'
prediction = svd.predict(user, item)
print(f"\nПредсказанная оценка для {user} и фильма '{item}': {prediction.est:.2f}")

# Функция для получения топ-N рекомендаций для пользователя
def get_top_n_recommendations(predictions, n=2):
    # Сначала отфильтруем оценки по интересующему нас пользователю
    top_n = {}
    for uid, iid, true_r, est, _ in predictions:
        if uid not in top_n:
            top_n[uid] = []
        top_n[uid].append((iid, est))
    
# Создаем и обучаем модель SVD
surprise_model = SVD(n_factors=50, n_epochs=30)
surprise_model.fit(trainset)

surprise_time = time.time() - start_time
print(f"Время обучения Surprise: {surprise_time:.2f} секунд")

# Оценим качество модели на тестовых данных
predictions = surprise_model.test(testset)
surprise_rmse_score = rmse(predictions)
print(f"RMSE для Surprise: {surprise_rmse_score:.4f}")

# 3. LightFM --------------------------------------------------------------
from lightfm import LightFM
from lightfm.data import Dataset as LFMDataset
from lightfm.evaluation import auc_score, precision_at_k

print("\n--- Тестирование LightFM ---")
# Преобразуем данные для LightFM
lfm_dataset = LFMDataset()
lfm_dataset.fit(
    users=data['userId'].unique(),
    items=data['movieId'].unique(),
)

# Добавим жанры фильмов как признаки предметов
genres = set()
for g in movies['genres'].str.split('|'):
    genres.update(g)

# Построим взаимодействия
train_interactions = lfm_dataset.build_interactions(
    [(row['userId'], row['movieId'], row['rating']) for _, row in train_data.iterrows()]
)[0]

# Подготовим матрицу признаков предметов (жанры фильмов)
item_features = []
for _, row in movies.iterrows():
    movie_genres = row['genres'].split('|')
    item_features.extend([(row['movieId'], genre) for genre in movie_genres])

item_features_matrix = lfm_dataset.build_item_features(
    [(movie_id, [feature]) for movie_id, feature in item_features]
)

# Замеряем время обучения
start_time = time.time()

# Создаем и обучаем модель LightFM
lightfm_model = LightFM(loss='warp')
lightfm_model.fit(
    interactions=train_interactions,
    item_features=item_features_matrix,
    epochs=30,
    verbose=False
)

lightfm_time = time.time() - start_time
print(f"Время обучения LightFM: {lightfm_time:.2f} секунд")

# Оценим качество модели
auc = auc_score(
    lightfm_model, 
    train_interactions,
    item_features=item_features_matrix,
).mean()
precision = precision_at_k(
    lightfm_model, 
    train_interactions, 
    k=5,
    item_features=item_features_matrix,
).mean()

print(f"AUC для LightFM: {auc:.4f}")
print(f"Precision@5 для LightFM: {precision:.4f}")

# Сравнение времени обучения
print("\n--- Сравнение времени обучения ---")
times = [implicit_time, surprise_time, lightfm_time]
names = ['Implicit', 'Surprise', 'LightFM']

for name, t in zip(names, times):
    print(f"{name}: {t:.2f} секунд")

# Вывод рекомендаций для примера
print("\n--- Пример рекомендаций ---")

# Выберем пользователя для примера
example_user_id = list(user_ids.keys())[0]
example_user_idx = user_ids[example_user_id]

print(f"Рекомендации для пользователя {example_user_id}:")

# Implicit рекомендации
print("\nImplicit рекомендации:")
implicit_recs = implicit_model.recommend(example_user_idx, train_sparse[example_user_idx], N=5)
for movie_idx, score in implicit_recs:
    movie_id = movie_map_reverse[movie_idx]
    movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]
    print(f"{movie_title}: {score:.4f}")

# Surprise рекомендации
print("\nSurprise рекомендации:")
# Получаем все фильмы, которые пользователь еще не оценил
user_rated_movies = set(train_data[train_data['userId'] == example_user_id]['movieId'])
all_movies = set(movies['movieId'])
unrated_movies = all_movies - user_rated_movies

# Предсказываем оценки для неоцененных фильмов
surprise_preds = []
for movie_id in unrated_movies:
    pred = surprise_model.predict(example_user_id, movie_id)
    surprise_preds.append((movie_id, pred.est))

# Выводим топ-5 рекомендаций
surprise_preds.sort(key=lambda x: x[1], reverse=True)
for movie_id, score in surprise_preds[:5]:
    movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]
    print(f"{movie_title}: {score:.4f}")

# LightFM рекомендации
print("\nLightFM рекомендации:")
# Получаем маппинг пользователей и фильмов в LightFM
lfm_user_map, lfm_item_map = lfm_dataset.mapping()
lfm_user_idx = lfm_user_map[example_user_id]

# Получаем количество фильмов
n_items = len(lfm_item_map)

# Получаем оценки для всех фильмов
scores = lightfm_model.predict(
    user_ids=[lfm_user_idx] * n_items,
    item_ids=list(range(n_items)),
    item_features=item_features_matrix
)

# Получаем топ-5 фильмов
top_items = np.argsort(-scores)[:5]

# Преобразуем обратно в ID фильмов и выводим результаты
for idx in top_items:
    # Находим ID фильма по индексу
    for movie_id, movie_idx in lfm_item_map.items():
        if movie_idx == idx:
            movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]
            print(f"{movie_title}: {scores[idx]:.4f}")
            break
```

Этот код демонстрирует, как использовать все три библиотеки для работы с одним и тем же набором данных MovieLens. Обратите внимание на различия в:
1. Подготовке данных для каждой библиотеки
2. Скорости обучения моделей
3. Метриках качества
4. Получаемых рекомендациях

Изучите каждую библиотеку глубже, чтобы выбрать наиболее подходящую для ваших конкретных задач.

Удачи в освоении библиотек для рекомендательных систем! 